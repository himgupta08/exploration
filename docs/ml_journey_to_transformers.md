![NLP_evolution](NLP_evolution.png)


### Rule-Based Systems (1950s — 1960s)
Use-cases: Translation services  
In 1954, IBM conducted research to translate 60 Russian sentences into English using a set of hand-crafted rules.

### Statistical Models (1960s — 2010s)
Algos: Hidden Markov Model (HMM), Text Mining algos (Tokenization, POS tagging, TF-IDF etc.)
Latent Dirichlet Allocation  
Use-cases: Text analysis, Sentiment analysis, Topic modeling, named entity recognition etc.  

### Neural networks and Deep Learning (2000s — 2020s)
Algos: Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), LSTMs  
Use cases: Language modeling, Word embedding, Sequence to Sequence Learning  

### Transformer Models (Mid-2010s — present)  
Algos: BERT, GPT  
Use Cases:Language Modeling, Question Answering, Text Generation  

#### How do they solve existing problems: 
Traditional RNNs were slow and computationally expensive.  
Transformer models can process all input text through parallelization, rather than one word at a time like RNNs (sequential inputs). Also, they require much less time to re-train.  

3 principals:  
Positional encoding  
Attention  
Self attention  